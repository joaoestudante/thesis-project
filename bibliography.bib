@ARTICLE{Alali_2008,title={What's a Typical Commit? A Characterization of Open Source Software Repositories},year={2008},author={Abdulkareem Alali and Huzefa Kagdi and Jonathan I. Maletic},doi={10.1109/icpc.2008.24},pmid={null},pmcid={null},mag_id={2150198410},journal={2008 16th IEEE International Conference on Program Comprehension},abstract={The research examines the version histories of nine open source software systems to uncover trends and characteristics of how developers commit source code to version control systems (e.g., subversion). The goal is to characterize what a typical or normal commit looks like with respect to the number of files, number of lines, and number of hunks committed together. The results of these three characteristics are presented and the commits are categorized from extra small to extra large. The findings show that approximately 75\% of commits are quite small for the systems examined along all three characteristics. Additionally, the commit messages are examined along with the characteristics. The most common words are extracted from the commit messages and correlated with the size categories of the commits. It is observed that sized categories can be indicative of the types of maintenance activities being performed.}}
@ARTICLE{Ying_2004,title={Predicting source code changes by mining change history},year={2004},author={Annie T. T. Ying and Gail C. Murphy and Raymond T. Ng and Mark C. Chu-Carroll},doi={10.1109/tse.2004.52},pmid={null},pmcid={null},mag_id={2133961160},journal={IEEE Transactions on Software Engineering},abstract={Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.}}
@ARTICLE{Kagdi_2007,title={Mining software repositories for traceability links},year={2007},author={Huzefa Kagdi and Jonathan I. Maletic and Bonita Sharif},doi={10.1109/icpc.2007.28},pmid={null},pmcid={null},mag_id={2117944944},journal={15th IEEE International Conference on Program Comprehension (ICPC '07)},abstract={An approach to recover/discover traceability links between software artifacts via the examination of a software system's version history is presented. A heuristic-based approach that uses sequential-pattern mining is applied to the commits in software repositories for uncovering highly frequent co-changing sets of artifacts (e.g., source code and documentation). If different types of files are committed together with high frequency then there is a high probability that they have a traceability link between them. The approach is evaluated on a number of versions of the open source system KDE. As a validation step, the discovered links are used to predict similar changes in the newer versions of the same system. The results show highly precision predictions of certain types of traceability links.}}
@ARTICLE{Gousios_2013,title={The GHTorent dataset and tool suite},year={2013},author={Georgios Gousios},doi={10.1109/msr.2013.6624034},pmid={null},pmcid={null},mag_id={1972386298},journal={2013 10th Working Conference on Mining Software Repositories (MSR)},abstract={During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.}}
@ARTICLE{Thongtanunam_2015,title={Who should review my code? A file location-based code-reviewer recommendation approach for Modern Code Review},year={2015},author={Patanamon Thongtanunam and Chakkrit Tantithamthavorn and Raula Gaikovina Kula and Norihiro Yoshida and Hajimu Iida and Ken-ichi Matsumoto},doi={10.1109/saner.2015.7081824},pmid={null},pmcid={null},mag_id={1998900885},journal={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},abstract={Software code review is an inspection of a code change by an independent third-party developer in order to identify and fix defects before an integration. Effectively performing code review can improve the overall software quality. In recent years, Modern Code Review (MCR), a lightweight and tool-based code inspection, has been widely adopted in both proprietary and open-source software systems. Finding appropriate code-reviewers in MCR is a necessary step of reviewing a code change. However, little research is known the difficulty of finding code-reviewers in a distributed software development and its impact on reviewing time. In this paper, we investigate the impact of reviews with code-reviewer assignment problem has on reviewing time. We find that reviews with code-reviewer assignment problem take 12 days longer to approve a code change. To help developers find appropriate code-reviewers, we propose RevFinder, a file location-based code-reviewer recommendation approach. We leverage a similarity of previously reviewed file path to recommend an appropriate code-reviewer. The intuition is that files that are located in similar file paths would be managed and reviewed by similar experienced code-reviewers. Through an empirical evaluation on a case study of 42,045 reviews of Android Open Source Project (AOSP), OpenStack, Qt and LibreOffice projects, we find that RevFinder accurately recommended 79\% of reviews with a top 10 recommendation. RevFinder also correctly recommended the code-reviewers with a median rank of 4. The overall ranking of RevFinder is 3 times better than that of a baseline approach. We believe that RevFinder could be applied to MCR in order to help developers find appropriate code-reviewers and speed up the overall code review process.}}
@ARTICLE{Gong_2021,title={Code Authors Hidden in File Revision Histories: An Empirical Study},year={2021},author={Siyi Gong and Hao Zhong and Hao Zhong},doi={10.1109/icpc52881.2021.00016},pmid={null},pmcid={null},mag_id={3174550694},journal={2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC)},abstract={Although many programmers write their names in the comments of a source file, from such comments, it is unreliable to identify code authors, since the modifications of many programmers are not recorded. Even if they are recorded in a code repository, many authors are hidden in revision histories.The true authors of source files are important in many research topics. For example, when detecting plagiarism, if the authors of two source are overlapped, it becomes more challenging to determine plagiarism than the source files that are written by individual authors. As it is difficult to determine true authors of a source file, researchers typically use source files whose authors are already known (e.g., the source files from Google Code Jam), but such files are not many and less representative. Meanwhile, although some empirical studies touch code authors, to the best of our knowledge, no prior study has analyzed the characteristics of code authors that are hidden in revision histories. As a result, many research questions along with code authors are still open. For example, how many authors does a source file can have, and what are the proportions of contributions per source file, if they are written by more than one author?To answer the timely questions, in this paper, we conducted an empirical study on code authors that are hidden in revision histories. To support our study, we implemented a tool called CODA. By comparing the latest code lines with past commits, CODA identifies the true authors of all code lines. With its support, we analyzed 12,092 source files that were written by 506 programmers. Our study answers several interesting questions concerning code authors. For example, we find that 75.4\% source files are written by multiple authors, and their contributions follow the famous 80/20 principle. These findings are useful to understand authors of source files in open source communities.}}
@ARTICLE{Yu_2016,title={Reviewer recommendation for pull-requests in GitHub},year={2016},author={Yue Yu and Huaimin Wang and Gang Yin and Tao Wang},doi={10.1016/j.infsof.2016.01.004},pmid={null},pmcid={null},mag_id={2288177242},journal={Information \& Software Technology},abstract={Context: The pull-based model, widely used in distributed software development, offers an extremely low barrier to entry for potential contributors (anyone can submit of contributions to any project, through pull-requests). Meanwhile, the project's core team must act as guardians of code quality, ensuring that pull-requests are carefully inspected before being merged into the main development line. However, with pull-requests becoming increasingly popular, the need for qualified reviewers also increases. GitHub facilitates this, by enabling the crowd-sourcing of pull-request reviews to a larger community of coders than just the project's core team, as a part of their social coding philosophy. However, having access to more potential reviewers does not necessarily mean that it's easier to find the right ones (the "needle in a haystack" problem). If left unsupervised, this process may result in communication overhead and delayed pull-request processing.Objective: This study aims to investigate whether and how previous approaches used in bug triaging and code review can be adapted to recommending reviewers for pull-requests, and how to improve the recommendation performance.Method: First, we extend three typical approaches used in bug triaging and code review for the new challenge of assigning reviewers to pull-requests. Second, we analyze social relations between contributors and reviewers, and propose a novel approach by mining each project's comment networks (CNs). Finally, we combine the CNs with traditional approaches, and evaluate the effectiveness of all these methods on 84 GitHub projects through both quantitative and qualitative analysis.Results: We find that CN-based recommendation can achieve, by itself, similar performance as the traditional approaches. However, the mixed approaches can achieve significant improvements compared to using either of them independently.Conclusion: Our study confirms that traditional approaches to bug triaging and code review are feasible for pull-request reviewer recommendations on GitHub. Furthermore, their performance can be improved significantly by combining them with information extracted from prior social interactions between developers on GitHub. These results prompt for novel tools to support process automation in social coding platforms, that combine social (e.g., common interests among developers) and technical factors (e.g., developers' expertise).}}
@ARTICLE{Balachandran_2013,title={Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation},year={2013},author={Vipin Balachandran},doi={10.1109/icse.2013.6606642},pmid={null},pmcid={null},mag_id={2151979607},journal={null},abstract={Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93\% of all the automatically generated comments. There is only 14.71\% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60\%-92\%, which is significantly better than a comparable method based on file change history.}}
@ARTICLE{Yang_2017,title={Stack overflow in github: any snippets there?},year={2017},author={Di Yang and Pedro Martins and Pedro Martins and Pedro Martins and Pedro Martins and Pedro Martins and Vaibhav Saini and Cristina V. Lopes},doi={10.1109/msr.2017.13},pmid={null},pmcid={null},mag_id={2952962030},journal={2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)},abstract={When programmers look for how to achieve certain programming tasks, Stack Overflow is a popular destination in search engine results. Over the years, Stack Overflow has accumulated an impressive knowledge base of snippets of code that are amply documented. We are interested in studying how programmers use these snippets of code in their projects. Can we find Stack Overflow snippets in real projects? When snippets are used, is this copy literal or does it suffer adaptations? And are these adaptations specializations required by the idiosyncrasies of the target artifact, or are they motivated by specific requirements of the programmer? The large-scale study presented on this paper analyzes 909k non-fork Python projects hosted on Github, which contain 290M function definitions, and 1.9M Python snippets captured in Stack Overflow. Results are presented as quantitative analysis of block-level code cloning intra and inter Stack Overflow and GitHub, and as an analysis of programming behaviors through the qualitative analysis of our findings.}}
@ARTICLE{Hauff_2015,title={Matching GitHub developer profiles to job advertisements},year={2015},author={Claudia Hauff and Georgios Gousios},doi={10.1109/msr.2015.41},pmid={null},pmcid={null},mag_id={2044675594},journal={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},abstract={GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally "be seen" by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.}}
@ARTICLE{Schuler_2008,title={Mining usage expertise from version archives},year={2008},author={David Schuler and Thomas Zimmermann},doi={10.1145/1370750.1370779},pmid={null},pmcid={null},mag_id={2104342335},journal={MSR '08},abstract={In software development, there is an increasing need to find and connect developers with relevant expertise. Existing expertise recommendation systems are mostly based on variations of the Line 10 Rule: developers who changed a file most often have the most implementation expertise. In this paper, we introduce the concept of usage expertise, which manifests itself whenever developers are using functionality, e.g., by calling API methods. We present preliminary results for the ECLIPSE project that demonstrate that our technique allows to recommend experts for files with no or little history, identify developers with similar expertise, and measure the usage of API methods.}}
@ARTICLE{Gousios_2008,title={Measuring developer contribution from software repository data},year={2008},author={Georgios Gousios and Eirini Kalliamvakou and Diomidis Spinellis},doi={10.1145/1370750.1370781},pmid={null},pmcid={null},mag_id={2163683660},journal={MSR '08},abstract={Apart from source code, software infrastructures supporting agile and distributed software projects contain traces of developer activity that does not directly affect the product itself but is important for the development process. We propose a model that, by combining traditional contribution metrics with data mined from software repositories, can deliver accurate developer contribution measurements. The model creates clusters of similar projects to extract weights that are then applied to the actions a developer performed on project assets to extract a combined measurement of the developer's contribution. We are currently implementing the model in the context of a software quality monitoring system while we are also validating its components by means of questionnaires.}}
@ARTICLE{Anvik_2006,title={Who should fix this bug},year={2006},author={John Anvik and Lyndon Hiew and Gail C. Murphy},doi={10.1145/1134285.1134336},pmid={null},pmcid={null},mag_id={2079317829},journal={ICSE},abstract={Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57\% and 64\% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.}}
@inproceedings{Horschig_2018,
author = {Horschig, Siegfried and Mattis, Toni and Hirschfeld, Robert},
title = {Do Java Programmers Write Better Python? Studying off-Language Code Quality on GitHub},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3214341},
doi = {10.1145/3191697.3214341},
abstract = {There are style guides and best practices for many programming languages. Their goal
is to promote uniformity and readability of code, consequentially reducing the chance
of errors. While programmers who are frequently using the same programming language
tend to internalize most of its best practices eventually, little is known about what
happens when they casually switch languages and write code in a less familiar language.
Insights into the factors that lead to coding convention violations could help to
improve tutorials for programmers switching languages, make teachers aware of mistakes
they might expect depending on what language students have been using before, or influence
the order in which programming languages are taught. To approach this question, we
make use of a large-scale data set representing a major part of the open source development
activity happening on GitHub. In this data set, we search for Java and C++ programmers
that occasionally program Python and study their Python code quality using a lint
tool. Comparing their defect rates to those from Python programmers reveals significant
effects in both directions: We observe that some of Python's best practices have more
widespread adoption among Java and C++ programmers than Python experts. At the same
time, python-specific coding conventions, especially indentation, scoping, and the
use of semicolons, are violated more frequently. We conclude that programming off-language
is not generally associated with better or worse code quality, but individual coding
conventions are violated more or less frequently depending on whether they are more
universal or language-specific. We intend to motivate a discussion and more research
on what causes these effects, how we can mitigate or use them for good, and which
related effects can be studied using the presented data set.},
booktitle = {Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming},
pages = {127–134},
numpages = {8},
keywords = {github, explorative study, best practices, code quality, lint},
location = {Nice, France},
series = {Programming'18 Companion}
}
@ARTICLE{Seker_2020,title={A Systematic Mapping of Software Engineering Challenges: GHTorrent Case.},year={2020},author={Abdulkadir Seker and Banu Diri and Halil Arslan and Halil Arslan and Halil Arslan and Mehmet Fatih Amasyali and Mehmet Fatih Amasyali},doi={null},pmid={null},pmcid={null},mag_id={3013848127},journal={arXiv: Software Engineering},abstract={Git is used as the distributed version control system for many open-source software projects. One Git-based service, GitHub, is the most common code hosting and repository service for open-source software projects. For researchers that study software engineering, the content that is hosted on these platforms provides much valuable data. There are some alternatives to get GitHub data such as GitHub Archive, GitHub API or GHTorrent. Among these options, GHTorrent is the most widely known and used GitHub dataset in the literature. Although there are some review studies about software engineering challenges across the GitHub platform, no review of GHTorrent dataset-specific research is available. In this study, the 172 studies that use GHTorrent as a data source were categorized within the scope of software engineering challenges and a systematic mapping study was carried out. Moreover, the pros and cons of the dataset have been indicated and the focused issues of the literature on and the open challenges have been noted.}}
