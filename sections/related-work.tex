\section{Related work}
\subsection{Extracting data from software repositories}

Mining repositories to extract knowledge from them has been an object of study for several years.

In \cite{Ying_2004} it is proposed an approach that, when developers are editing a file, recommends other files that may also need edits. This is done by analyzing which files have frequently changed together in the past (using a version control system), and applying a frequent pattern mining algorithm to them. For a better analysis, they consider not only files that were committed together, but also files that were committed by the same author in a short period of time, as they most likely refer to the same task. Finally, they also filter commits with too many files changed, as these commits usually don't refer to a single task and so aren't relevant for recommendations.

A frequent pattern mining algorithm is also used in \cite{Kagdi_2007}. The knowledge of files frequently changed together (called "change sets") is used to uncover "traceability between source code and other artifacts", like documentation. They also use heuristics like changes in a small period of time ("\textbf{time-interval}"), changes by the same author ("\textbf{committer}"), and a mix of both ("\textbf{time-interval + committer}") to group change sets that probably refer to the same task/change. By evaluating the approach on several versions of \textit{KDE} (K Desktop Environment), the authors find that this approach is able to find links between different types of artifacts like "source code files, change logs, user documentation, and build files" with high accuracy.

In \cite{Alali_2008}, an analysis of different metrics of commits like number of changed files, number of changed lines and hunks (which are groups of lines that changed, together with contextual lines that did not) is done to categorize a commit from "extra small to extra large". The categorization of a single commit is always done relative to predefined criteria, obtained from all commits of the same repository. This predefined criteria is obtained (for each metric) by calculating the statistical 5-point summary (Minimum, Q1, Median, Q3, Maximum). Plotting these values into a Boxplot allows for a division into 5 regions, as seen in figure \ref{fig:boxplot-commit-size}. A single commit is then classified according to where the metrics fall in this Boxplot.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.2]{imgs/boxplot-commit-size.png}
    \caption{Boxplot of the regions for classification of commit size. From \cite{Alali_2008}}
    \label{fig:boxplot-commit-size}
\end{figure}

Applying this to the commits of nine open source repositories (of various sizes and languages), and using messages in commits' log files to determine the activity performed, let the authors conclude that there is some evidence the category of a commit can indicate the activity being performed. For example, extra-small commits typically have the set of words \{add, bug\} in their messages.

The problem of assigning the best code reviewer(s) to a given pull request affects many players in the industry, with \cite{Yu_2016} mentioning that "the management of pull requests is identified as the most important project activity on GitHub". Different authors try to solve this problem with past software history change analysis. A tool called "Review Bot" was developed in \cite{Balachandran_2013} mainly to aid code reviewers in the process of reviewing pull requests, by using static analysis techniques to check for code style violations and bad patterns. The tool also recommends code reviewers based on \textit{line change history}, and it does so with an accuracy of 60\% to 90\%. Another approach to recommend reviewers, but based on file paths, is described in \cite{Thongtanunam_2015}. The tool they propose is applied to a new review, and the recommendations are based on who reviewed similar file paths in the past as the ones changed in that review. Finding similar file paths is done by string comparison techniques, like \textbf{Longest Common Prefix} (files under the same directory are considered similar), \textbf{Longest Common Suffix} (files with the same name are considered similar), \textbf{Longest Common Substring} and \textbf{Longest Common Subsequence} (files under a similar directory structure are similar). This approach was evaluated on a total of 42,045 reviews across 5 different open source projects (Android Open Source Project (AOSP), OpenStack, Qt and LibreOffice), and it "correctly
recommended 79\% of reviews with a top 10 recommendation" - 3 times better than ReviewBot.


Both of these papers address the issue by ultimately assuming that developers who changed a file most often have the most familiarity and expertise in that file. However, \cite{Schuler_2008} assume that developers who use a \textit{functionality} most often, by calling a method, are the most familiar with it. They introduce the notion of \textit{usage based expertise}, which, together with \textit{implementation based expertise} (those who modified methods are most familiar with them), is used to create \textbf{expertise profiles}. To build this profile for a developer, they looked at and counted all the methods that the developer changed (or added) and used in past commits. Then, it's easy to find developers who have experience with using a given module - it's a matter of considering a set with all methods of that module, and counting intersections with profiles. One application mentioned in the paper is in the calculation of neighborhoods of developers - sets of developers with the most similar expertise. This could help with the creation of teams (with members from different projects) to discuss usage of an API, or with scheduling meetings with developers working on "related parts of the code", or even "to choose developers for training of their skills", by identifying who would need training in a given technology the most.


There are also other interesting conclusions and associations that can be extracted with the data of different repositories. Not much is known regarding what happens to the code quality of programmers very familiar with a language when they switch to another they don't use that often. This is explored in \cite{Horschig_2018}, where the authors searched in a GitHub dataset for Java and C++ programmers that occasionally program in Python, and studied their Python code quality with a lint tool. They found, for example, that C++ programmers usually write more complex methods in Python than a control group, and they tend to use loop variables outside of a loop more often. Java programmers tend to redefine outer names more times than Python programmers, and both C++ and Java programmers write longer lines.
% Este paper do Horschig usou o GHTorrent. Será preferível mencionar isso, e meter o GHTorrent num parágrafo anterior?

Stack Overflow (SO) is an ever-present website in developers' lives, with many snippets available for solving both common and unique problems. In \cite{Yang_2017}, the presence of SO snippets in real projects, and what adaptations (if any) these snippets had is studied. They analyzed 909k non-fork Python projects, and found that the percentage of snippets found both in SO and GitHub (GH) is low - under 2\% of analyzed snippets, which results in just thousands of snippets. Nevertheless, there is some evidence of code flow from SO to GH.


Measuring developer contribution in an agile and distributed development context is not a trivial task. According to \cite{Gousios_2008}, "an important portion of development time is spent on communication and manipulation of development support tools". Therefore, a new model for contribution measurement was developed and explained, which mixes repository data with traditional contribution metrics (lines of code) to get a more accurate contribution measure. The model is based on a function that applies weights to different actions on a repository. Committing translation files, including a bug report number on a commit message, or committing documentation files are examples of code-related actions. However, there are also non code-related actions, like creating or updating a new Wiki page, participating in IRC (keep in mind the paper was published in 2008), replying to a mailing list thread, or closing a bug. All of these are associated with positive weights, but there are some actions with negative weights - closing a bug that is then reopened, committing without messages, making large commits (with more than \textit{X} files), or making commits that generate bugs. To compute the weights, the authors first compute clusters of similar projects, since certain types of projects are expected to have more actions of certain types than others. Then, all of the considered events are counted across all projects of each cluster, and the weight $w_i$ for each action is "the percentage of contribution of each action category to the total number of actions". Manual weights are applied to the sum of all $w_i \times action_i$ for actions of a given type. These manual weights are chosen according to the needs of each project. This offers a contribution factor that is applied to developers and is summed to their total Lines of Code (LOC), resulting in a modern contribution metric.


%According to \cite{Gong_2021}, code authorship attribution is an important research topic, with applications ranging from bug report assignments and software forensics to plagiarism detection. In the same paper, the revision histories and past contributions of developers are leveraged to try to find hidden code authors. This was done with an empirical evaluation, by developing a tool called CodA which evaluated in total 12092 files written by 506 programmers, in six Java projects. In total, they reported 7 findings. For example, if all lines written by author $A$ in source file $S$ are replaced in later commits, then it's not legitimate to say that $A$ is still an author of $S$.


\cite{Gousios_2013} released the "GHTorrent Project", also available online\footnote{\url{https://ghtorrent.org/}}. Through a decentralized way, all data from GitHub events (like new commits, new authors, new issues - everything that may happen in a repository) is stored in a standardized form in a database.
This way, researchers can scale their findings to many repositories, as well as combine data from different repositories more easily, as it's a matter of performing queries on a database rather than using an API. The paper also presents several research opportunities, like ...

\cite{Seker_2020} investigated 172 studies that used this database, and categorized them into several software engineering domains and the challenges they addressed in those domains. The domains considered and some of their challenges are found below:

\begin{itemize}
    \item User
        \begin{itemize}
            \item \textit{Activity}. Covers developer's contributions, like coding history, comments, and stars in repositories.
            \item \textit{Revision - Assignment}. Everything that deals with pull request (PR) reviewer or bug assignment.
        \end{itemize}
    \item Development
        \begin{itemize}
            \item \textit{Pull Request and Quality}. Covers problems with pull request prioritization, and acceptance/rejection and reasons for it.
        \end{itemize}
    \item Project
        \begin{itemize}
            \item \textit{Issue/bug}. Includes topics of open and closed issues in a project and bug occurrences, as well as the classification of issue-bug-feature.
            \item \textit{Dependency}. Contains examinations of dependencies on GitHub projects on programming languages, codes, forking cases...
        \end{itemize}
    \item Dataset
        \begin{itemize}
            \item \textit{Helper}. Studies that used only a few features of GHTorrent to solve dissimilar problems.
            \item \textit{Subset}. Studies created by filtering the dataset according to certain features of developers or projects.
        \end{itemize}
\end{itemize}

The study also presented several pros and cons of GHTorrent, as well as challenges found using it. For example, GHTorrent is very flexible as it presents data in different formats (JSON in a MongoDB database and relational tables in MySQL database). However, it contains some duplicated data, it is missing some fields that could link data (like the \textit{repo id} in commits), and it does not have data on who edited what file.

\subsection{Summary}
Table \ref{tab:software-history-usage} summarizes what particular part of the software repositories the previously described studies use.

% Áreas são genéricas o suficiente?
\begin{table}[h]
    \centering
    \caption{Summary of the usage of software history}
    \begin{tabular}{|P|L|L|L|}
        \hline
         Paper & Information Used & Granularity & Area \\
         \hline
         \cite{Ying_2004} & Files frequently changed together, Committer, Time of commit & File & File editing recommendation \\
         \hline
         \cite{Kagdi_2007} & Files frequently changed together, Committer, Time of commit & File & Uncover traceability links \\
         \hline
         \cite{Alali_2008} & Commits & File, Lines & Commit categorization \\
         \hline
         \cite{Balachandran_2013} & Commits & Lines & Pull request reviewer assignment \\
         \hline
         \cite{Thongtanunam_2015} & Commits & Changed files' path & Pull request reviewer assignment \\
         \hline
         \cite{Schuler_2008} & Commits & Methods & Pull request reviewer assignment, Others \\
         \hline
         \cite{Horschig_2018} & Files & File & Off-Language Code Quality \\
         \hline
         \cite{Yang_2017} & Files & Lines & Code flow from StackOverflow to GitHub \\
         \hline
         \cite{Gousios_2008} & Commits, Mailing lists/Forum, Bug Database, Wiki, IRC & File, Thread Replies, Participation & Developer contribution \\
         \hline
    \end{tabular}
    \label{tab:software-history-usage}
\end{table}

\subsection{Related work in migration} % Titulo WIP

% identificar o state of the art nas migrações (com o paper do science direct)

% aprofundar mais os que usam dados de repos